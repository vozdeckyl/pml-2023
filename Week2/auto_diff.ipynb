{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397d97f3",
   "metadata": {},
   "source": [
    "# Automatic differentiation (AD) and gradient tape #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ecbaa6",
   "metadata": {},
   "source": [
    "This notebook gives a short overlook at how to compute weight derivatives using automatic differentiation. It uses tensorflow's GradientTape to effectively compute all gradients required by the backward pass procedure. First we look at the basic implementation of Gradient tape, then apply it to linear regression: optimising (using gradient descent or Adam) the loss function so we can learn the weights associated to the straight-line-with-noise data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d299cad",
   "metadata": {},
   "source": [
    "## Gradient tape ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8311a59",
   "metadata": {},
   "source": [
    "Evaluate the cell below to import the packages and to initialise some tensorflow variables used in the derivative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x1 = tf.Variable(3.0, name=\"x1\")\n",
    "x2 = tf.Variable(5.0, name=\"x2\")\n",
    "x3 = tf.Variable(6.0, name=\"x3\")\n",
    "\n",
    "def f(a,b):\n",
    "    return a**2 + b\n",
    "\n",
    "with tf.GradientTape() as tape: # remove/insert persistent=True in the argument to see issue.\n",
    "  y = f(x1,x2)**2 + 5*x3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5303446",
   "metadata": {},
   "source": [
    "Gradient tape records the necessary derivative operations for the function \"on a tape\". This is achieved using the ``with tf.GradientTape() as tape`` line, followed by the function we wish to take derivatives of. Setting this up, we can calculate the value of the derivative of ``y`` with respect to input variable ``x1``. Evaluate the cell below to see this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradx1 = tape.gradient(y,x1)\n",
    "print(gradx1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f4d18",
   "metadata": {},
   "source": [
    "Unless we specify ``persistent=True`` in the GradientTape argument, we can only make one gradient calculation. Evaluate the next cell to see the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fb28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradx2 = tape.gradient(y,x2)\n",
    "print(gradx2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8cfaa",
   "metadata": {},
   "source": [
    "Update the argument with ``persistent=True`` and run the previous cells above once more. You should get the derivative of ``y`` with respect to ``x2`` this time. **Repeat the same idea for the x3 variable derivative in the cell below. Does the numerical output make sense?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c319519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0335bc7",
   "metadata": {},
   "source": [
    "We must use trainable variables if we want gradient tape to watch/recognise them when establishing derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = (x0**2) + (x1**2) + (x2**2) + (x3**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "  print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f4379",
   "metadata": {},
   "source": [
    "The ``watched_variables()`` method allows us to check which variables are being watched on the tape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "[var.name for var in tape.watched_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96af1b8",
   "metadata": {},
   "source": [
    "As a simple exercise, we now check the gradient tape calculations do indeed adhere to standard calculus procedures like chain rule. Execute the cells below to see the print out of these derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1950c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([1, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "\n",
    "print(\"dz/dx = \", tape.gradient(z, x).numpy()) \n",
    "print(\"dy/dx = \",tape.gradient(y, x).numpy()) \n",
    "print(\"dz/dy = \",tape.gradient(z, y).numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b02f324",
   "metadata": {},
   "source": [
    "Another example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([1, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y = 3*x**2\n",
    "  z = tf.sin(y) + 2*y\n",
    "\n",
    "print(\"dz/dx = \", tape.gradient(z, x).numpy())  \n",
    "print(\"dy/dx = \",tape.gradient(y, x).numpy()) \n",
    "print(\"dz/dy = \",tape.gradient(z, y).numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee1dc5a",
   "metadata": {},
   "source": [
    "**Is it clear how to obtain dz/dx by the chain rule multiplication? If you know how, symbolically differentiate the function z as a function of x for each example, then insert the numerical values (1 and 3) into the result to check the answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe0fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f11b58c",
   "metadata": {},
   "source": [
    "## Linear regression, GradientTape and AD ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f811acb",
   "metadata": {},
   "source": [
    "Here we present another route of computing the line of best fit (linear regression) as we saw in the Linear Regression notebook. The process is in essence identical, but here we explictly call on GradientTape rather than use Keras. We borrow part of the code from the previous exercise in order to create some line data with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92683fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# # If plots are not being outputted and Kernel quitting, try uncommenting the two lines below.\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\"\"\"\n",
    "Generate the data to be fitted\n",
    "    xmin      Minimum value in x to sample\n",
    "    xmax      Maximum value in x to sample\n",
    "    Ntrain    Number of train data to generate\n",
    "    Ntest     Number of test data to generate\n",
    "    m         gradient for the line\n",
    "    c         constant offset\n",
    "    Noise     (fractional) Noise level to generate\n",
    "\"\"\"\n",
    "xmin   = -10\n",
    "xmax   = 10\n",
    "Ntrain = 100\n",
    "Ntest  = 100\n",
    "m      = 10.0\n",
    "c      = 5.0\n",
    "Noise  = 0.05\n",
    "\n",
    "def genData(xmin, xmax, Ntrain, Ntest, m, c, Noise):\n",
    "    \"\"\"\n",
    "    Function to generate an ensemble of test and train data for fitting\n",
    "    \"\"\"\n",
    "    print(\"\\033[92mGenerating the parabola data set\\033[0m\") #The format is the colour of the words\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test  = []\n",
    "    y_test  = []\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    def sim_line(xmin, xmax, m, c, Noise):\n",
    "        \"\"\"\n",
    "        Function to simulate a random data point for a straight line\n",
    "        \"\"\"\n",
    "        x = np.random.random()*(xmax-xmin)+xmin\n",
    "        y = (m*x+c)*(1 + random.random()*Noise)\n",
    "    \n",
    "        return x, y\n",
    "    #--------------------------------------------------------------------\n",
    "  \n",
    "    for i in range( Ntrain ):\n",
    "        x,y = sim_line(xmin, xmax, m, c, Noise)\n",
    "        x_train.append([x])\n",
    "        y_train.append([y])\n",
    "\n",
    "    for i in range( Ntest ):\n",
    "        x,y = sim_line(xmin, xmax, m, c, Noise)\n",
    "        x_test.append([x])\n",
    "        y_test.append([y])\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# generate data for fitting\n",
    "x_train, y_train, x_test, y_test = genData(xmin, xmax, Ntrain, Ntest, m, c, Noise)\n",
    "\n",
    "print(\"Have generated:\")\n",
    "print(\"\\tN(train) examples            = \", len(x_train))\n",
    "print(\"\\tN(test) examples             = \", len(x_test))\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a601d",
   "metadata": {},
   "source": [
    "Let us plot the training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c20f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68596f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking some functions used below...\n",
    "?tf.random.normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e89032-4ff9-4488-9a92-cad9c06278e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.ones_like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d149d",
   "metadata": {},
   "source": [
    "Running the cell below optimises the weight $m$ and bias $c$ (packaged into a variable we call ``w``) using Adam. Have a read through and check you get the gist of things :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e6bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packaging the data in a simple vector/matrix form\n",
    "\n",
    "Y = y_train\n",
    "XX = np.hstack([x_train, np.ones_like(x_train)]) # Creating a stacked array for easy multiplication with the weight vector\n",
    "\n",
    "# Prepare TensorFlow objects\n",
    "w = tf.Variable(tf.random.normal((2,1)), name=\"m_and_c\")# Packaging the weights (m,c) into a single, randomly initialised variable\n",
    "x = tf.constant(XX, dtype=tf.float32)     # input sample array\n",
    "y = tf.constant(Y, dtype=tf.float32)      # output sample sample array\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "print(w)\n",
    "\n",
    "# Run optimizer\n",
    "# The underscore is just a placeholder for the iteration variable since we do not use one explictly in the code.\n",
    "mse_list=[]\n",
    "for _ in range(3000):\n",
    "    with tf.GradientTape(persistent=True) as tape: # Use persistent=True so we can take derivatives of any node quantity\n",
    "        Sigma = x @ w # Graph node 1. Also x@w signifies matrix mutliplcation: every x becomes dot producted with weights w.\n",
    "                      # So Sigma is a vector with i'th component (m x_{i} + c). Note c in previous sentence is also an Ntrain length vector \n",
    "                      # with all components c\n",
    "        y_pred = Sigma # Graph node 2. A kind of redundant addition, but it at least helps us connect with the graph nodes.\n",
    "        mse = (1/Ntrain)*tf.reduce_sum(tf.square(y-y_pred)) # Graph node 3.\n",
    "        print(mse)\n",
    "        mse_list.append(mse) # Collecting the mse values for each loop into a list.\n",
    "        \n",
    "    grad = tape.gradient(mse, w) # Here we take the gradient of the mse with respect to w=(weight,bias)=(m,c) - the main goal\n",
    "                                 # in AD.\n",
    "    grad1 = tape.gradient(mse, Sigma)    # The tape will also create/evaluate these values automatically, \n",
    "                                         # but by choosing persistent=True, we\n",
    "    grad2 = tape.gradient(mse, y_pred)   # can manually create/calculate them ourselves.\n",
    "    \n",
    "    \n",
    "    optimizer.apply_gradients([(grad, w)]) # Finally we optimize. \n",
    "\n",
    "print(\"The final weight array w = (m,c): \")\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The final weight array w = (m,c): \")\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20637083",
   "metadata": {},
   "source": [
    "Note how the linear activation function step within the tape, given by ``y_pred = Sigma``, is pretty much redundant since we could just use ``y_pred = x @ w`` from the get go. To reiterate, this is just to establish the graph nodes as we did in the talk. For example, in another task such as binary classification, we could replace ``y_pred = Sigma`` with ``y_pred = tf.keras.activations.sigmoid(Sigma)`` instead. \n",
    "\n",
    "Above, we printed the value of the MSE to see how it was changing. **Type ``plt.plot(mse_list)`` below and evaluate it to visualise how the loss function changes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4ab63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de659768",
   "metadata": {},
   "source": [
    "**Using your final output weight and bias, both stored in variable ``w``, construct a line from these and plot it on top of the test data - using x_test and y_test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9cd8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f17d7a06",
   "metadata": {},
   "source": [
    "Finally, let us look at the gradients we calculated. Evaluate the cell below. Again, each of these gradients are $\\frac{\\partial L}{\\partial v} = \\bar{v}$ where $v$ is either $\\hat{y} \\equiv y_{\\text{pred}}$, $\\Sigma$ or $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3761a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad)\n",
    "print(grad1)\n",
    "print(grad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f01842",
   "metadata": {},
   "source": [
    "**What can you say about the shape of these gradients?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207c4be",
   "metadata": {},
   "source": [
    "It may help to look at each of these quantities in index form below. Here, $i$ indexes the sample set, running from 1 to $N_{train}$.\n",
    "\n",
    "Forward pass creates the quantities at each graph node:\n",
    "$$\\Sigma_{i} = m x_{i} + c$$\n",
    "\n",
    "$$\\hat{y}_{i} = \\Sigma_{i}$$\n",
    "\n",
    "$$L = \\frac{1}{N_{train}} \\sum_{i=1}^{N_{train}} (y_{i}- \\hat{y}_{i})^2 = \\frac{1}{N_{train}} \\sum_{i=1}^{N_{train}} ( y_{i}-(m x_{i} + c) )^2 $$\n",
    "\n",
    "where $\\hat{y} \\equiv y_{pred}$. \n",
    "\n",
    "On the backward pass the GradientTape calculates all necessary the derivatives. The ones we explicitly save in ``grad``, ``grad1`` and ``grad2`` are \n",
    "\n",
    "``grad`` = $$ \\bar{w} = (\\bar{m}, \\bar{c}) = \\left(\\frac{\\partial L}{\\partial m}, \\quad \\frac{\\partial L}{\\partial c}\\right) = \\left(-\\frac{2}{N_{train}} \\sum_{i=1}^{N_{train}} x_{i}( y_{i}-(m x_{i} + c)) , \\quad -\\frac{2}{N_{train}} \\sum_{i=1}^{N_{train}}(y_{i}-(m x_{i} + c )) \\right)$$\n",
    "\n",
    "``grad1`` = $$ \\bar{ \\Sigma}_{i} = \\frac{\\partial L}{\\partial \\Sigma_{i}} = - \\frac{2}{N_{train}} \\left( y_{i} - \\Sigma_{i} \\right)$$\n",
    "\n",
    "``grad2`` = $$\\bar{ \\hat{y}_{i}} = \\frac{\\partial L}{\\partial \\hat{y}_{i}} = - \\frac{2}{N_{train}} \\left( y_{i} - \\hat{y}_{i}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a57062",
   "metadata": {},
   "source": [
    "## References ##\n",
    "- 1/. For the reference code and further description seen in this notebook: https://machinelearningmastery.com/using-autograd-in-tensorflow-to-solve-a-regression-problem/\n",
    "- 2/. For tensorflows introduction to autodifferentiation: https://www.tensorflow.org/guide/autodiff#:~:text=TensorFlow%20provides%20the%20tf.,GradientTape%20onto%20a%20%22tape%22.\n",
    "- 3/. Further notes from Roger Grosse on autodifferentiation, See lecture L06 backpropagation: https://sgfin.github.io/files/notes/CS321_Grosse_Lecture_Notes.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
