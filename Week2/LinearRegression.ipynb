{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearRegression  - This is a linear regregssion example using Keras\n",
    "\n",
    "    Copyright (C) 2020 Adrian Bevan, Queen Mary University of London\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "    \n",
    "----------------------\n",
    "\n",
    "LinearRegression example using a Keras model. This is a simple example $y = mx+c$ fitting example that uses an Adam optimiser.  For more information about this optimiser please see the original paper by Kingma and Ba, [arXiv:1412.6980](https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "----------------------\n",
    "## Generating the data\n",
    "\n",
    "To generate the data we randomly sample the domain $x = [xmin, xmax]$, and the noise in this case\n",
    "is assumed to be relative to the magnitude of the signal (i.e. y value).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# # If plots are not being outputted and Kernel quitting, try uncommenting the two lines below.\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generate the data to be fitted\n",
    "    xmin      Minimum value in x to sample\n",
    "    xmax      Maximum value in x to sample\n",
    "    Ntrain    Number of train data to generate\n",
    "    Ntest     Number of test data to generate\n",
    "    m         gradient for the line\n",
    "    c         constant offset\n",
    "    Noise     (fractional) Noise level to generate\n",
    "\"\"\"\n",
    "xmin   = -10\n",
    "xmax   = 10\n",
    "Ntrain = 100\n",
    "Ntest  = 100\n",
    "m      = 1.0\n",
    "c      = 0.0\n",
    "Noise  = 0.1\n",
    "\n",
    "def genData(xmin, xmax, Ntrain, Ntest, m, c, Noise):\n",
    "    \"\"\"\n",
    "    Function to generate an ensemble of test and train data for fitting\n",
    "    \"\"\"\n",
    "    print(\"\\033[92mGenerating the parabola data set\\033[0m\")\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test  = []\n",
    "    y_test  = []\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    def sim_line(xmin, xmax, m, c, Noise):\n",
    "        \"\"\"\n",
    "        Function to simulate a random data point for a parabola\n",
    "        \"\"\"\n",
    "        x = random.random()*(xmax-xmin)+xmin\n",
    "        y = (m*x+c)*(1 + random.random()*Noise)\n",
    "    \n",
    "        return x, y\n",
    "    #--------------------------------------------------------------------\n",
    "  \n",
    "    for i in range( Ntrain ):\n",
    "        x,y = sim_line(xmin, xmax, m, c, Noise)\n",
    "        x_train.append(x)\n",
    "        y_train.append(y)\n",
    "\n",
    "    for i in range( Ntest ):\n",
    "        x,y = sim_line(xmin, xmax, m, c, Noise)\n",
    "        x_test.append(x)\n",
    "        y_test.append(y)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# generate data for fitting\n",
    "x_train, y_train, x_test, y_test = genData(xmin, xmax, Ntrain, Ntest, m, c, Noise)\n",
    "\n",
    "print(\"Have generated:\")\n",
    "print(\"\\tN(train) examples            = \", len(x_train))\n",
    "print(\"\\tN(test) examples             = \", len(x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## Building the model\n",
    "\n",
    "Having generated the data we will now build a model for the optimisation problem. For this we need to make a Dense layer with a linear activation function.  If we are interested in tuning intial parameters for this model, then that can accelerate the optimisation process; but here we use the nominal values assigned when randomly initialised.\n",
    "\n",
    "The hyper parameters of the model are a single weight and a single bias, i.e. $m$ and $c$, respectively. These parameters need to be optimised, and here we use the Mean Square Error loss function. i.e. $$MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (y_i - \\widehat{y}(x_i))$$ where $\\widehat{y}$ is the linear model being fitted, and n is the number of training examples used for the optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Training configuration\n",
    "#\n",
    "Nepochs         = 20\n",
    "learning_rate   = 0.5\n",
    "\n",
    "#   Build the model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add( tf.keras.layers.Dense(1, activation='linear', input_shape=[1,]) )\n",
    "\n",
    "#   Now specify the loss function - MSE\n",
    "loss_fn = tf.keras.losses.MSE\n",
    "\n",
    "#   Now we can train the model to make predictions.\n",
    "#   Use the ADAM optimiser\n",
    "#   Specify the metrics to report as accuracy\n",
    "#   Specify the loss function (see above)\n",
    "#   the fit step specifies the number of training epochs\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=opt, loss=loss_fn )\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\033[92mWill perform a linear regression optimisation\\033[0m\")\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Input data Linear Regression Data\")\n",
    "print(\"Nepochs              = \", Nepochs)\n",
    "print(\"learning rate        = \", learning_rate)\n",
    "print(\"Optimser             =  Adam\")\n",
    "print(\"Loss function        =  MSE\")\n",
    "\n",
    "history  = model.fit( x_train, y_train, epochs=Nepochs)\n",
    "\n",
    "#    Print out the history keys expected are:\n",
    "#    loss        The loss function evaluated at each epoch for the training set\n",
    "#    acc         The accuracy evaluated at each epoch for the training set\n",
    "print(\"history keys = \", history.history.keys())\n",
    "\n",
    "print(\"Display the evolution of the loss as a function of the training epoch\")\n",
    "print(\"  N(Epochs)                = \", Nepochs)\n",
    "print(\"  Initial loss (train)     = {:5.4f}\".format( history.history['loss'][0]) )\n",
    "print(\"  Final loss (train)       = {:5.4f}\".format( history.history['loss'][-1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call a summary of our network using model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Note:}$ There is alternative way to write a neural network, all within the sequential bracket. We can add more layers this way by adding a comma followed by the next layer. For a many layered network this can be quicker as we do not require model.add numerous times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential(\n",
    "[\n",
    "    tf.keras.layers.Dense(1, activation='linear', input_shape=[1,])\n",
    "])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Output\n",
    "\n",
    "For this optimisation we care about understanding the evolution of the loss function, but even when this plateaus we may find that the prediction of the model is not perfect. The ultimate goal is to be able obtain a model that fits the data well.  The optimisation process does not involve determination of uncertainties for the parameters, unlike a least squares computation where these can be solved for analyticailly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "print(\"\\n\\033[1mPlotting the loss function evolution as a function of training epoch\\033[0m\\n\")\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# plot the model for the data\n",
    "# we are interested in the first layer\n",
    "layer = model.layers[0]\n",
    "weights, biases = layer.get_weights()\n",
    "ymin = weights[0][0]*xmin+biases[0]\n",
    "ymax = weights[0][0]*xmax+biases[0]\n",
    "\n",
    "print(\"\\tm = \", weights[0][0])\n",
    "print(\"\\tc = \", biases[0])\n",
    "print(\"\\tMSE loss = \".format( history.history['loss'][-1]) )\n",
    "print(\"\\t Fitted line xrange = {:}, y range = {:}\".format([xmin, xmax], [ymin, ymax]))\n",
    "\n",
    "print(\"\\n\\033[1mPlotting the optimised model and test data\\033[0m\\n\")\n",
    "plt.plot(x_test, y_test, \"r.\")\n",
    "plt.plot([xmin, xmax], [ymin, ymax], \"b-\")  # plot the fitted line\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.legend(\"test data\", \"fitted model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## Suggested exercises:\n",
    "\n",
    " - Change the number of training examples to see how this affects the optimisation performance (increase by a factor of 10 and decrease by a factor of 10).\n",
    " - Change the value of m and c to extract, Try $m=10$, $c=5$, to explore how this affects the training.  You may also need to change the number of epochs when doing this.\n",
    " - Change the number of training epochs to see how this affects the optimisation\n",
    " - Change the noise level to study how this affects the optimisation.\n",
    " - Change the learning rate to explore how robust the training is with the Adam optmiser.\n",
    " - You may also wish to explore the use of other optmisers: see https://keras.io/api/optimizers/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
